#+STARTUP: hidestars
#+STARTUP: indent

#+author: F. Dangel

* Combining [[https://deepobs.readthedocs.io/en/stable/][DeepOBS]] and [[https://backpack.readthedocs.io/en/latest/][BackPACK]]
For an overview about the supported problems, run
#+BEGIN_SRC bash :results output
  python example/supported.py 
#+END_SRC

#+RESULTS:
#+begin_example
Supported:
	✔ cifar10_3c3d
	✔ cifar10_vgg16
	✔ cifar10_vgg19
	✔ cifar100_3c3d
	✔ cifar100_allcnnc
	✔ cifar100_vgg16
	✔ cifar100_vgg19
	✔ fmnist_2c2d
	✔ fmnist_logreg
	✔ fmnist_mlp
	✔ mnist_2c2d
	✔ mnist_logreg
	✔ mnist_mlp
	✔ quadratic_deep
	✔ svhn_3c3d
Not supported:
	❌ cifar100_wrn164
	❌ cifar100_wrn404
	❌ fmnist_vae
	❌ mnist_vae
	❌ svhn_wrn164
#+end_example

Unsupported problems include:
- Variational autoencoder problems (~mnist_vae~, ~fmnist-vae~)
- Problems with batch normalization (~cifar100_wrn164~, ~cifar100_wrn404~, ~svhn_wrn164~)

** Installation
#+BEGIN_SRC bash
pip install -e git://github.com/f-dangel/backobs.git@master#egg=backobs
#+END_SRC
** Basic examples
- How to extend a testproblem in [[https://github.com/fsschneider/DeepOBS][DeepOBS]] with [[https://www.backpack.pt][BackPACK]] ([[file:./example/extend.py][file]]):
  #+BEGIN_SRC bash :results output
    python example/extend.py
  #+END_SRC

  #+RESULTS:
#+begin_example
dense.weight
	.grad.shape:              torch.Size([10, 784])
	.grad_batch.shape:        torch.Size([128, 10, 784])
	.variance.shape:          torch.Size([10, 784])
	.sum_grad_squared.shape:  torch.Size([10, 784])
	.batch_l2.shape:          torch.Size([128])
	.diag_ggn_mc.shape:       torch.Size([10, 784])
	.diag_ggn_exact.shape:    torch.Size([10, 784])
	.diag_h.shape:            torch.Size([10, 784])
	.kfac (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
	.kflr (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
	.kfra (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
dense.bias
	.grad.shape:              torch.Size([10])
	.grad_batch.shape:        torch.Size([128, 10])
	.variance.shape:          torch.Size([10])
	.sum_grad_squared.shape:  torch.Size([10])
	.batch_l2.shape:          torch.Size([128])
	.diag_ggn_mc.shape:       torch.Size([10])
	.diag_ggn_exact.shape:    torch.Size([10])
	.diag_h.shape:            torch.Size([10])
	.kfac (shapes):           [torch.Size([10, 10])]
	.kflr (shapes):           [torch.Size([10, 10])]
	.kfra (shapes):           [torch.Size([10, 10])]
#+end_example
- How to extend a testproblem with [[https://www.backpack.pt][BackPACK]] and get access to the unreduced loss ([[file:./example/extend_with_access_unreduced_loss.py][here]]):
  #+BEGIN_SRC bash :results output
    python example/extend_with_access_unreduced_loss.py
  #+END_SRC

  #+RESULTS:
  #+begin_example
  Individual loss shape:    torch.Size([128])
  Mini-batch loss:          tensor(2.3026, device='cuda:0', grad_fn=<AddBackward0>)
  Averaged individual loss: tensor(2.3026, device='cuda:0')
  dense.weight
    .grad.shape:              torch.Size([10, 784])
    .grad_batch.shape:        torch.Size([128, 10, 784])
    .variance.shape:          torch.Size([10, 784])
    .sum_grad_squared.shape:  torch.Size([10, 784])
    .batch_l2.shape:          torch.Size([128])
    .diag_ggn_mc.shape:       torch.Size([10, 784])
    .diag_ggn_exact.shape:    torch.Size([10, 784])
    .diag_h.shape:            torch.Size([10, 784])
    .kfac (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
    .kflr (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
    .kfra (shapes):           [torch.Size([10, 10]), torch.Size([784, 784])]
  dense.bias
    .grad.shape:              torch.Size([10])
    .grad_batch.shape:        torch.Size([128, 10])
    .variance.shape:          torch.Size([10])
    .sum_grad_squared.shape:  torch.Size([10])
    .batch_l2.shape:          torch.Size([128])
    .diag_ggn_mc.shape:       torch.Size([10])
    .diag_ggn_exact.shape:    torch.Size([10])
    .diag_h.shape:            torch.Size([10])
    .kfac (shapes):           [torch.Size([10, 10])]
    .kflr (shapes):           [torch.Size([10, 10])]
    .kfra (shapes):           [torch.Size([10, 10])]
  #+end_example
- A [[https://github.com/fsschneider/DeepOBS][DeepOBS]] testproblem runner for SGD, extended with [[https://www.backpack.pt][BackPACK]] functionality ([[file:./example/runner.py][here]]):
  #+BEGIN_SRC bash :results output
    python example/run.py mnist_logreg --lr 0.1
  #+END_SRC

  #+RESULTS:
  #+begin_example
  ,********************************
  Evaluating after 0 of 1 epochs...
  TRAIN: loss 2.30259, acc 0.104610
  VALID: loss 2.30259, acc 0.100510
  TEST: loss 2.30259, acc 0.098010
  ,********************************
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 10, 784)
    Parameter 1: Shape (3, 10)
  ,********************************
  Evaluating after 1 of 1 epochs...
  TRAIN: loss 2.83062, acc 0.176418
  VALID: loss 2.81457, acc 0.180918
  TEST: loss 2.84024, acc 0.178218
  ,********************************
  #+end_example

** Important
- ℓ₂ regularization is *not supported*:
  #+BEGIN_SRC bash :results output
    # ℓ₂ not supported: this will crash!
    # python example/run.py cifar10_3c3d --lr 0.1

    # ℓ₂ disabled: works
    python example/run.py cifar10_3c3d --lr 0.1 --l2_reg 0.0
  #+END_SRC

  #+RESULTS:
  #+begin_example
  Files already downloaded and verified
  Files already downloaded and verified
  Files already downloaded and verified
  ,********************************
  Evaluating after 0 of 1 epochs...
  TRAIN: loss 2.32498, acc 0.101810
  VALID: loss 2.32938, acc 0.096710
  TEST: loss 2.32596, acc 0.100110
  ,********************************
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  [backobs] Extension: BatchGrad
    Parameter 0: Shape (3, 64, 3, 5, 5)
    Parameter 1: Shape (3, 64)
    Parameter 2: Shape (3, 96, 64, 3, 3)
    Parameter 3: Shape (3, 96)
    Parameter 4: Shape (3, 128, 96, 3, 3)
    Parameter 5: Shape (3, 128)
    Parameter 6: Shape (3, 512, 1152)
    Parameter 7: Shape (3, 512)
    Parameter 8: Shape (3, 256, 512)
    Parameter 9: Shape (3, 256)
    Parameter 10: Shape (3, 10, 256)
    Parameter 11: Shape (3, 10)
  ,********************************
  Evaluating after 1 of 1 epochs...
  TRAIN: loss 2.35165, acc 0.100910
  VALID: loss 2.34386, acc 0.101110
  TEST: loss 2.34491, acc 0.101110
  ,********************************
  #+end_example

* Details and debug info (TODO, ignore everything below)
** Preliminaries 
*** Set up a virtual environment
**** Anaconda
Use the ~.yml~ file to install all required dependencies into a ~conda~ environment named ~backobs~.
#+BEGIN_SRC bash
conda env create -f .conda_env.yml
#+END_SRC
Activate it
#+BEGIN_SRC bash
conda activate backobs
#+END_SRC
**** I don't want to use Anaconda
Make sure you have ~backpack~ and ~deepobs~ installed in your favorite environment.

*** Install ~backobs~
#+BEGIN_SRC bash
pip install -e git://github.com/f-dangel/backobs.git@master#egg=backobs
#+END_SRC
** Example
*** The runner
The [[file:example/][example directory]] contains a [[file:example/runner.py][basic runner]] that integrates BackPACK into DeepOBS test problems. You can modify it to your needs, or leave it as is to run an example.
*** Run SGD on a DeepOBS problem with BackPACK 
There exists a [[file:example/run.py][run script]] you can simply execute from the command line.

Let's run SGD on the MNIST linear regression task with a learning rate of ~0.1~ and momentum of ~0.9~:
#+BEGIN_SRC bash
python example/run.py mnist_logreg --lr 0.1 --momentum 0.9
#+END_SRC

*** Not all problems are supported
ResNets and variational autoencoders are not supported by BackPACK.

For instance, this will crash:
#+BEGIN_SRC 
# WideResNet
python example/run.py svhn_wrn164 --lr 0.1 --momentum 0.9
#+END_SRC

